{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embedding_tools as et\n",
    "import visualization as viz\n",
    "import data_loading as dl\n",
    "import risk_pricing as rp\n",
    "import utilities as ut\n",
    "import mpfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading utils/mpfiles/__init__.py\n",
      "Loading utilities\n",
      "Loading utils/embedding_tools/__init__.py\n",
      "Loading utils/embedding_tools/__init__.py\n",
      "Loading all the dirty 1As from the year 2006\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Loading all the dirty 1As from the year 2007\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Loading all the dirty 1As from the year 2008\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Loading all the dirty 1As from the year 2009\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Created 7000 texts so far\n",
      "Created 7500 texts so far\n",
      "Created 8000 texts so far\n",
      "Loading all the dirty 1As from the year 2010\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Created 7000 texts so far\n",
      "Created 7500 texts so far\n",
      "Created 8000 texts so far\n",
      "Loading all the dirty 1As from the year 2011\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Created 7000 texts so far\n",
      "Created 7500 texts so far\n",
      "Created 8000 texts so far\n",
      "Loading all the dirty 1As from the year 2012\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Created 7000 texts so far\n",
      "Created 7500 texts so far\n",
      "Loading all the dirty 1As from the year 2013\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Created 7000 texts so far\n",
      "Created 7500 texts so far\n",
      "Loading all the dirty 1As from the year 2014\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Created 7000 texts so far\n",
      "Created 7500 texts so far\n",
      "Loading all the dirty 1As from the year 2015\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Created 7000 texts so far\n",
      "Created 7500 texts so far\n",
      "Loading all the dirty 1As from the year 2016\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Created 7000 texts so far\n",
      "Loading all the dirty 1As from the year 2017\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Loading all the dirty 1As from the year 2018\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Loading all the dirty 1As from the year 2019\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Loading all the dirty 1As from the year 2020\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Loading all the dirty 1As from the year 2021\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Loading all the dirty 1As from the year 2022\n",
      "Creating texts\n",
      "Created 500 texts so far\n",
      "Created 1000 texts so far\n",
      "Created 1500 texts so far\n",
      "Created 2000 texts so far\n",
      "Created 2500 texts so far\n",
      "Created 3000 texts so far\n",
      "Created 3500 texts so far\n",
      "Created 4000 texts so far\n",
      "Created 4500 texts so far\n",
      "Created 5000 texts so far\n",
      "Created 5500 texts so far\n",
      "Created 6000 texts so far\n",
      "Created 6500 texts so far\n",
      "Dataset initialized\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import importlib\n",
    "#import embedding_tools as et\n",
    "import embedding_tools as et\n",
    "import pandas as pd\n",
    "importlib.reload(et)\n",
    "dog = et.Dataset()\n",
    "dataset = dog.from_texts()\n",
    "\n",
    "#%%\n",
    "print(\"Dataset initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading utils/embedding_tools/__init__.py\n",
      "Loaded all the filtered texts, CIKs and years from the pickle files\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(et)\n",
    "dataset = et.Dataset().from_pkls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(et)\n",
    "dataset.create_embeddings(2016, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A_files_cleaned/2022/embeddings2022.npy\":\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "embeddings = np.load(\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A_files_cleaned/2022/embeddings2022.npy\")\n",
    "# Load \"clean_texts_filter2022.pkl\":\n",
    "clean_texts_filter = pd.read_pickle(\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A_files_cleaned/2022/clean_texts_filter2022.pkl\")\n",
    "# Load \"clean_texts2022.pkl\":\n",
    "clean_texts = pd.read_pickle(\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A_files_cleaned/2022/clean_texts2022.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts_filter = pd.read_pickle(\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A_files_cleaned/2012/clean_texts_filter2012.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the years you want to process\n",
    "years = range(2006, 2022 + 1)\n",
    "\n",
    "# Initialize empty lists to store embeddings, years, and CIKs\n",
    "all_embeddings = []\n",
    "all_years = []\n",
    "all_ciks = []\n",
    "\n",
    "# Loop through each year to load the embeddings and CIKs\n",
    "for year in years:\n",
    "    # Load embeddings\n",
    "    if year == 2012 or year == 2006:\n",
    "        embeddings_file_path = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A_files_cleaned/{year}/embeddings{year}.pkl\"\n",
    "        embeddings = pd.read_pickle(embeddings_file_path)\n",
    "        embeddings = np.array(embeddings)\n",
    "        all_embeddings.append(embeddings)\n",
    "    elif year > 2012:\n",
    "        embeddings_file_path = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A_files_cleaned/{year}/embeddings{year}.npy\"\n",
    "        embeddings = np.load(embeddings_file_path)\n",
    "        all_embeddings.append(embeddings)\n",
    "    \n",
    "    # Load CIKs\n",
    "    pickle_file_path = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A_files_cleaned/{year}/clean_texts_filter{year}.pkl\"\n",
    "    clean_texts_filter = pd.read_pickle(pickle_file_path)\n",
    "    ciks = clean_texts_filter['cik']\n",
    "    \n",
    "    # Append embeddings, years, and CIKs to the lists\n",
    "    \n",
    "    all_years.append(np.full(ciks.shape[0], year))\n",
    "    all_ciks.append(ciks)\n",
    "\n",
    "# Concatenate all embeddings, years, and CIKs along the first axis (rows)\n",
    "all_embeddings_concatenated = np.concatenate(all_embeddings, axis=0)\n",
    "all_years_concatenated = np.concatenate(all_years, axis=0)\n",
    "all_ciks_concatenated = np.concatenate(all_ciks, axis=0)\n",
    "\n",
    "# Create a DataFrame from the concatenated embeddings\n",
    "embedding_df = pd.DataFrame(all_embeddings_concatenated)\n",
    "\n",
    "# Add the year and CIK columns to the DataFrame\n",
    "embedding_df['year'] = all_years_concatenated\n",
    "embedding_df['CIK'] = all_ciks_concatenated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your DataFrame (assuming embedding_df is already loaded)\n",
    "# embedding_df = pd.read_csv('path_to_your_embeddings.csv')  # or however you load your DataFrame\n",
    "\n",
    "# Extract the embedding columns (all columns except 'year' and 'CIK')\n",
    "embedding_columns = embedding_df.columns[:-2]  # Assumes 'year' and 'CIK' are the last two columns\n",
    "\n",
    "# Extract the data to cluster\n",
    "embedding_data = embedding_df[embedding_columns].values\n",
    "\n",
    "# Elbow Method to determine the optimal number of clusters\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "# K should vary from 6 to 20 with steps of 2:\n",
    "K = range(6, 21, 2)  # Try different numbers of clusters from 6 to 20\n",
    "\n",
    "for k in K:\n",
    "    print(f\"Running KMeans with {k} clusters\")\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(embedding_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(embedding_data, kmeans.labels_))\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(K, inertia, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal Number of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Plot the Silhouette Scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(K, silhouette_scores, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores For Optimal Number of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal number of clusters based on the plots\n",
    "optimal_k = K[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et.run_fa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample data to demonstrate the process\n",
    "# Replace this with your actual data loading code\n",
    "# embedding_df = pd.read_csv('your_data_file.csv')\n",
    "\n",
    "# Preserve the columns 'year' and 'CIK'\n",
    "columns_to_preserve = ['year', 'CIK']\n",
    "data_for_clustering = embedding_df.drop(columns=columns_to_preserve)\n",
    "\n",
    "# Initialize KMeans with 10 clusters\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "\n",
    "# Fit and predict the clusters\n",
    "embedding_df['cluster'] = kmeans.fit_predict(data_for_clustering)\n",
    "\n",
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.rename(columns={\"cluster\": \"max_topic\"}, inplace=True)\n",
    "topic_map = embedding_df[['max_topic', 'year', 'CIK']]\n",
    "# Save topic_map as a CSV to /Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/embeddings_km10\n",
    "topic_map.to_csv(\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/embeddings_km10/topic_map_2006_2022.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import embedding_tools as et\n",
    "\n",
    "# Get embeddings for the words \"innovation\", \"intellectual property\", \"patent\", \"knowledge capital\", \"clinical trial\", and \"software\"\n",
    "inno_et = et.get_text_embeddings(\"innovation\")\n",
    "ip_et = et.get_text_embeddings(\"intellectual property\")\n",
    "patent_et = et.get_text_embeddings(\"patent\")\n",
    "knowledge_capital_et = et.get_text_embeddings(\"knowledge capital\")\n",
    "clinical_trial_et = et.get_text_embeddings(\"clinical trial\")\n",
    "software_et = et.get_text_embeddings(\"software\")\n",
    "\n",
    "# Stack embeddings into a matrix\n",
    "embeddings_matrix = np.vstack([inno_et, ip_et, patent_et, knowledge_capital_et, clinical_trial_et, software_et])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(cosine_sim, dtype=bool))\n",
    "\n",
    "# Create a heatmap with the cosine similarity, masking the upper triangle\n",
    "words = [\"innovation\", \"intellectual property\", \"patent\", \"knowledge capital\", \"clinical trial\", \"software\"]\n",
    "sns.heatmap(cosine_sim, annot=True, xticklabels=words, yticklabels=words, cmap='viridis', mask=mask)\n",
    "plt.title('Cosine Similarity between Word Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(et.get_text_embeddings(\"intellectual property\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import embedding_tools as et\n",
    "\n",
    "# Example embedding DataFrame (assuming you already have your DataFrame `embedding_df`)\n",
    "# embedding_df = pd.read_csv('your_embedding_dataframe.csv')\n",
    "\n",
    "# Get the embedding for the expression \"intellectual property\"\n",
    "ip_embedding = np.array(et.get_text_embeddings(\"intellectual property\")).reshape(1, -1)\n",
    "\n",
    "# Function to calculate cosine similarity between row vector A and ip_embedding B\n",
    "def calculate_cosine_similarity(row):\n",
    "    vector_a = row.values.reshape(1, -1)\n",
    "    return cosine_similarity(vector_a, ip_embedding)[0, 0]\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "embedding_df['ip_cs'] = embedding_df.iloc[:, 0:1536].apply(calculate_cosine_similarity, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(embedding_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the cosine similarity values\n",
    "# Rename ip_cs to topic_kk:\n",
    "embedding_df.rename(columns={\"ip_cs\": \"topic_kk\"}, inplace=True)\n",
    "# Save the updated DataFrame to a CSV file\n",
    "# Create directory if it doesn't exist:\n",
    "import os\n",
    "output_dir = \"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/embeddings_km10_ipcs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "embedding_df.rename(columns={\"cluster\": \"max_topic\"}, inplace=True)\n",
    "topic_map = embedding_df[['max_topic', 'topic_kk', 'year', 'CIK']]\n",
    "# Save topic_map as a CSV to /Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/embeddings_km10\n",
    "topic_map.to_csv(\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/embeddings_km10_ipcs/topic_map_2006_2022.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_visualization = embedding_df.drop(columns=['year', 'CIK'])\n",
    "\n",
    "# Perform PCA to reduce dimensions to 2\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data_for_visualization.drop(columns=['cluster']))\n",
    "\n",
    "# Create a DataFrame with the reduced data and the cluster information\n",
    "reduced_df = pd.DataFrame(reduced_data, columns=['PC1', 'PC2'])\n",
    "reduced_df['cluster'] = data_for_visualization['cluster']\n",
    "\n",
    "# Sample 500 points for visualization\n",
    "sampled_df = reduced_df.sample(n=500, random_state=42)\n",
    "\n",
    "# Plot the reduced data\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(sampled_df['PC1'], sampled_df['PC2'], c=sampled_df['cluster'], cmap='viridis', alpha=0.5)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Embedding Data (Sample of 500 points)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply PCA\n",
    "n_components = 20  # Adjust the number of components based on your needs\n",
    "scaler = StandardScaler()\n",
    "standardized_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(standardized_embeddings)\n",
    "\n",
    "# Retrieve the explained variance (eigenvalues)\n",
    "eigenvalues = pca.explained_variance_\n",
    "eigenvalues = eigenvalues[:100]\n",
    "# Plot the Scree Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, 'o-', markersize=8)\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Scree Plot for PCA')\n",
    "plt.axhline(y=1, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Determine the number of components using the Kaiser Criterion (eigenvalues > 1)\n",
    "n_components_kaiser = np.sum(eigenvalues > 1)\n",
    "print(f\"Number of components according to Kaiser Criterion: {n_components_kaiser}\")\n",
    "# Find the sum of the variance explained by the 20 first components:\n",
    "print(f\"Sum of the variance explained by the {n_components} first components:\", sum(pca.explained_variance_ratio_[:n_components]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envkkr)",
   "language": "python",
   "name": "envkkr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
