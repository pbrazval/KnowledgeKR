{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8a18d9",
   "metadata": {},
   "source": [
    "### The following packages were installed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e25dc",
   "metadata": {},
   "source": [
    "Other packages such as `spacy` had been installed before. This code was run in a venv in my MBP with Python 3.9 and Conda. \n",
    "\n",
    "Stopwords from English used.\n",
    "\n",
    "For reproducibility: I can just reload pickles at the end of the code (lemmatizing takes a long time)\n",
    "\n",
    "## st3f: removes drop_lowvalue_words_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44ad556-97df-4478-bb08-fee270d3de74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00b1538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in ./envkkr/lib/python3.9/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./envkkr/lib/python3.9/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in ./envkkr/lib/python3.9/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./envkkr/lib/python3.9/site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in ./envkkr/lib/python3.9/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./envkkr/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./envkkr/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./envkkr/lib/python3.9/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./envkkr/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./envkkr/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./envkkr/lib/python3.9/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./envkkr/lib/python3.9/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./envkkr/lib/python3.9/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./envkkr/lib/python3.9/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./envkkr/lib/python3.9/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./envkkr/lib/python3.9/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./envkkr/lib/python3.9/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./envkkr/lib/python3.9/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./envkkr/lib/python3.9/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./envkkr/lib/python3.9/site-packages (from spacy) (2.7.0)\n",
      "Requirement already satisfied: jinja2 in ./envkkr/lib/python3.9/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in ./envkkr/lib/python3.9/site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./envkkr/lib/python3.9/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./envkkr/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./envkkr/lib/python3.9/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./envkkr/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in ./envkkr/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./envkkr/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./envkkr/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./envkkr/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./envkkr/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./envkkr/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./envkkr/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./envkkr/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./envkkr/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./envkkr/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./envkkr/lib/python3.9/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement csv (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for csv\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement glob (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for glob\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: importlib in ./envkkr/lib/python3.9/site-packages (1.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyLDAvis in ./envkkr/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (1.26.4)\n",
      "Requirement already satisfied: scipy in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (1.5.3)\n",
      "Requirement already satisfied: joblib in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (3.1.3)\n",
      "Requirement already satisfied: numexpr in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (2.10.0)\n",
      "Requirement already satisfied: future in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (1.0.0)\n",
      "Requirement already satisfied: funcy in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: sklearn in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (0.0.post12)\n",
      "Requirement already satisfied: scikit-learn in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (1.4.0)\n",
      "Requirement already satisfied: gensim in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (4.3.2)\n",
      "Requirement already satisfied: setuptools in ./envkkr/lib/python3.9/site-packages (from pyLDAvis) (58.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./envkkr/lib/python3.9/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./envkkr/lib/python3.9/site-packages (from pandas>=1.2.0->pyLDAvis) (2024.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./envkkr/lib/python3.9/site-packages (from gensim->pyLDAvis) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./envkkr/lib/python3.9/site-packages (from jinja2->pyLDAvis) (2.1.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./envkkr/lib/python3.9/site-packages (from scikit-learn->pyLDAvis) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./envkkr/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in ./envkkr/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./envkkr/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./envkkr/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./envkkr/lib/python3.9/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in ./envkkr/lib/python3.9/site-packages (from nltk) (4.66.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim\n",
    "%pip install spacy\n",
    "%pip install csv\n",
    "%pip install glob\n",
    "%pip install importlib\n",
    "%pip install json\n",
    "%pip install pyLDAvis\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9367ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231c7db",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "%pip install pandas==1.5.3\n",
    "%pip install pyLDAvis==3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a81115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, gensim, glob, importlib, json, mpfiles, os, pickle, pyLDAvis, re, spacy, string, sys, warnings\n",
    "import numpy as np, pandas as pd, gensim.corpora as corpora, utilities as ut\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, TfidfModel\n",
    "from gensim.test.utils import datapath\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.test.utils import datapath\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "importlib.reload(mpfiles)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "min10kwords = 200\n",
    "filename_list = []\n",
    "\n",
    "def create_textsmp(yr):\n",
    "    print(f\"Creating texts\")\n",
    "    texts = []\n",
    "    i = 0\n",
    "    filename_list = []\n",
    "    results = []\n",
    "    for qtr in [1,2,3,4]:\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        print(f\"Pool started with {mp.cpu_count()} cores. Qtr {qtr}\")\n",
    "        for filename in glob.glob(f'/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A files/{yr}/Q{qtr}/*.txt'):\n",
    "            results.append(pool.apply_async(mpfiles.process_file, args=(filename,)))\n",
    "            i += 1\n",
    "            if i % 500 == 0:\n",
    "                print(f\"Created {i} texts so far\")\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    for r in results:\n",
    "        result = r.get()\n",
    "        filename, doc = result\n",
    "        filename_list.append(filename)    \n",
    "        texts.append(doc)                 \n",
    "    return texts, filename_list\n",
    "\n",
    "def lemmatization(old_texts, selection, yr, num_processes=mp.cpu_count()):\n",
    "    idxs_to_keep = selection['order_in_cik']\n",
    "    print(f\"Starting lemmatization\")\n",
    "    pool = mp.Pool(num_processes)\n",
    "    results = []\n",
    "    texts = [old_texts[i] for i in idxs_to_keep]\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(pool.apply_async(mpfiles.lemmatize_text, args=(text,)))\n",
    "        # Print progress: every 500 iterations print the iteration number\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f\"Lemmatized {i+1} texts so far\")\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    texts_out = [r.get() for r in results]\n",
    "    path = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/lemmatized_texts/{yr}\"\n",
    "# Create the directory if it doesn't exist\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    # Save the vector as a pickle file\n",
    "    with open(os.path.join(path, f\"lemmatized_texts{yr}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(texts_out, f)\n",
    "    \n",
    "    with open(os.path.join(path, f\"lem_filter{yr}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(selection, f)\n",
    "    \n",
    "    return texts_out\n",
    "\n",
    "def drop_lowvalue_words_parallel(corpus, tfidf, low_value, words_missing_in_tfidf):\n",
    "    \n",
    "    print(\"Dropping low value words.\")\n",
    "    num_processes = mp.cpu_count() - 1\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    chunk_size = int(len(corpus) / num_processes)\n",
    "    chunks = [corpus[i:i+chunk_size] for i in range(0, len(corpus), chunk_size)]\n",
    "    results = pool.map(partial(mpfiles.droplowvwords_chunk, tfidf=tfidf, low_value = low_value, words_missing_in_tfidf = [] ), chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Concatenate the results\n",
    "    new_corpus = [bow for chunk in results for bow in chunk]\n",
    "\n",
    "    return new_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84904a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "cpsrc = 'full'\n",
    "remake_id2word = False\n",
    "remake_lemmatization = False\n",
    "remake_ldamodel = False\n",
    "remake_multiyear_topic_map = False\n",
    "remake_corpus = False\n",
    "show_model = True\n",
    "no_below = 40 # Suppose 40256*0.001. Follows Bybee et al.\n",
    "no_above = 0.8 \n",
    "keep_n = None\n",
    "yearlist = [year for year in range(2006, 2023, 1)]\n",
    "min_count = 10 # For bigrams. Useless bigrams will be filtered out anyway\n",
    "thr = 10 # For bigrams. Useless bigrams will be filtered out anyway\n",
    "scorfun = \"default\" # For bigrams. Works best after trying the other type (npmi)\n",
    "numtopiclist = [6]\n",
    "#numtopiclist = [4] # Choose as you like.\n",
    "dicname = f\"dic{cpsrc}mc{min_count}thr{str(thr).replace('.', '_')}{scorfun[:3]}nob{no_below}noa{str(no_above).replace('.', '_')}\"\n",
    "dicpath = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/id2word/{dicname}.txt\"\n",
    "corpuspath = f'/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/corpora/{dicname}/corpus_full.mm' \n",
    "cequity_mapper = pd.read_csv(\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/input/cequity_mapper.csv\")\n",
    "list_of_words = ['patent', 'innovation', 'intellectual_property']\n",
    "if remake_lemmatization:    \n",
    "    for yr in yearlist:\n",
    "        print(f\"Starting year {yr}\")\n",
    "        texts, filename_list = create_textsmp(yr)\n",
    "        # Lemmatized texts still follow order in filename_list\n",
    "        selection, idxs_to_keep, ciks_to_keep = ut.filter_corpus(texts, filename_list, cequity_mapper, yr, min10kwords)\n",
    "        lemmatized_texts = lemmatization(texts, selection, yr)\n",
    "        filelist = glob.glob(f'/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/1A files/{yr}/*.txt')\n",
    "        ut.create_crosswalks(filelist, yr)\n",
    "        \n",
    "if remake_corpus:\n",
    "    if remake_id2word:\n",
    "        lemmatized_texts = []\n",
    "        yr_vec = []\n",
    "        idxs_to_keep = pd.Series()\n",
    "        ciks_to_keep = pd.Series()\n",
    "        for yr in yearlist:\n",
    "            file_path = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/lemmatized_texts/{yr}/lemmatized_texts{yr}.pkl\"\n",
    "            # Load the file using pickle\n",
    "            with open(file_path, 'rb') as f:\n",
    "                lemmatized_texts = lemmatized_texts + pickle.load(f)\n",
    "            filter_path = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/lemmatized_texts/{yr}/lem_filter{yr}.pkl\"\n",
    "            with open(filter_path, 'rb') as f:\n",
    "                selection = pickle.load(f)\n",
    "            idxs_to_keep = idxs_to_keep.append(selection['order_in_cik'])\n",
    "            ciks_to_keep = ciks_to_keep.append(selection['cik'])\n",
    "            yr_vec = yr_vec + [yr for _ in selection['cik']]\n",
    "        print(f\"Length of lemmatized_texts is: {len(lemmatized_texts)}\")\n",
    "        \n",
    "        os.makedirs(f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/corpora/{dicname}/\", exist_ok=True)\n",
    "        corpus_info = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/corpora/{dicname}/corpus_info.pkl\"\n",
    "        with open(corpus_info, \"wb\") as f:\n",
    "            pickle.dump((idxs_to_keep, ciks_to_keep, yr_vec), f)\n",
    "        data_bigrams_trigrams = ut.make_multigrams(lemmatized_texts, min_count = min_count, threshold = thr, scoring = scorfun)\n",
    "        id2word = ut.make_id2word(data_bigrams_trigrams, dicpath, no_below, no_above, keep_n)\n",
    "        print(f'I have just created a dictionary with length {len(id2word)}.')\n",
    "    else:\n",
    "        id2word = corpora.Dictionary.load_from_text(dicpath)\n",
    "    corpus, tfidf = ut.bow_texts(data_bigrams_trigrams, id2word)\n",
    "    #corpus = drop_lowvalue_words_parallel(corpus, tfidf, 0.03, [])\n",
    "    if not os.path.exists(f'/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/corpora/{dicname}'):\n",
    "        os.makedirs(f'/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/corpora/{dicname}')\n",
    "    MmCorpus.serialize(corpuspath, corpus)\n",
    "else:\n",
    "    corpus_info = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/corpora/{dicname}/corpus_info.pkl\"\n",
    "    with open(corpus_info, \"rb\") as f:\n",
    "        idxs_to_keep, ciks_to_keep, yr_vec = pickle.load(f)\n",
    "    corpus = MmCorpus(corpuspath)\n",
    "    id2word = corpora.Dictionary.load_from_text(dicpath)\n",
    "\n",
    "if remake_multiyear_topic_map:   \n",
    "    for num_topics in numtopiclist:\n",
    "        eta = create_eta(num_topics, id2word, list_of_words = list_of_words)\n",
    "        # Append timestamp to the model name:\n",
    "        # Find timestamp:\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = now.strftime(\"%Y%m%d%H%M\")\n",
    "        if eta == 'symmetric':\n",
    "            modelname = f\"{dicname}_{num_topics}_t\"\n",
    "        else:\n",
    "            modelname = f\"{dicname}_{num_topics}_{timestamp}_t\"\n",
    "        modelpath = datapath(f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/code/step_by_step/{modelname}\")\n",
    "        if remake_ldamodel:\n",
    "            print(f\"Making model {modelname}...\")\n",
    "            lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                                   eta = eta,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics,\n",
    "                                               random_state=100, passes = 10) #update_every=1,chunksize=100,passes=10,#alpha=\"auto\"\n",
    "            lda_model.save(modelpath)\n",
    "            lda_model.annotations = {'words_set_to_topic_0': list_of_words}\n",
    "            topics_per_doc = [lda_model.get_document_topics(doc) for doc in corpus]\n",
    "            print(f\"Model {modelname} created.\")\n",
    "            #         pyLDAvis.enable_notebook()\n",
    "            #         vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds = \"mmds\", R = 50)\n",
    "            #         vis\n",
    "        else:\n",
    "            print(f\"Retrieving model {modelname}...\")\n",
    "            lda_model = LdaModel.load(modelpath)\n",
    "            topics_per_doc = [lda_model[unseen_doc] for unseen_doc in corpus]\n",
    "\n",
    "        ut.print_coherence(lda_model, corpus, num_topics)\n",
    "        \n",
    "        if show_model:\n",
    "            pyLDAvis.enable_notebook()\n",
    "            vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds = \"mmds\", R = 50)\n",
    "            vis\n",
    "        ut.make_topicmap2(lda_model, topics_per_doc, yr_vec, ciks_to_keep, modelname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea14208",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "%pip install guidedlda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d054f1",
   "metadata": {},
   "source": [
    "# Create the bubble chart here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ffae0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds = \"mmds\", R = 50)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae38e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "num_topics = 4\n",
    "dicname = \"dicfullmc10thr10defnob40noa0_8\"\n",
    "modelname = f\"{dicname}_{num_topics}t\"\n",
    "corpuspath = f'/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/corpora/{dicname}/corpus_full.mm' \n",
    "dicpath = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/output/id2word/{dicname}.txt\"\n",
    "corpus = MmCorpus(corpuspath)\n",
    "id2word = corpora.Dictionary.load_from_text(dicpath)\n",
    "print(f'I am using a dictionary with length {len(id2word)}. What do you think?')\n",
    "modelpath = datapath(f\"/Users/pedrovallocci/Documents/PhD (local)/Research/By Topic/Measuring knowledge capital risk/code/step_by_step/{modelname}\")\n",
    "lda_model = LdaModel.load(modelpath)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds = \"mmds\", R = 50)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3263f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45904a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "lambd = 0.3 # a specific relevance metric value\n",
    "all_topics = {}\n",
    "num_topics = lda_model.num_topics \n",
    "num_terms = 30 \n",
    "\n",
    "for i in range(1,num_topics+1): \n",
    "    topic = vis.topic_info[vis.topic_info.Category == 'Topic'+str(i)].copy()\n",
    "    topic['relevance'] = topic['loglift']*(1-lambd)+topic['logprob']*lambd\n",
    "    all_topics['Topic '+str(i)] = topic.sort_values(by='relevance', ascending=False).Term[:num_terms].values\n",
    "all_topics_df = pd.DataFrame(all_topics)\n",
    "all_topics_df.columns = ['Topic_sw', 'Topic_rawm', 'Topic_finl', 'Topic_kk']\n",
    "\n",
    "topics = []\n",
    "for i in range(len(all_topics_df.columns)):\n",
    "    topics.append((i, [(word, 1) for word in all_topics_df.iloc[:, i]]))\n",
    "topics\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=2500,\n",
    "                  max_words=40,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "#topics = lda_model.show_topics(num_words = 40, formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title(all_topics_df.columns[i], fontdict=dict(size=14))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "folder_path = f\"/Users/pedrovallocci/Documents/PhD (local)/Research/Github/KnowledgeKRisk_10Ks/text/{modelname}/\"\n",
    "plt.savefig(folder_path + 'wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303eb262",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/pedrovallocci/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envkkr)",
   "language": "python",
   "name": "envkkr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
